{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sharp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sharp, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Linear(28*28, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 36),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(36, 18),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(18, 9),\n",
    "                )\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(9, 18),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(18, 36),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(36, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 28*28),\n",
    "                # nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Loss 0.111922\n",
      "Loss 0.066366\n",
      "Loss 0.056636\n",
      "Loss 0.065829\n",
      "Loss 0.058455\n",
      "Loss 0.052827\n",
      "Loss 0.045442\n",
      "Loss 0.048756\n",
      "Loss 0.050583\n",
      "Loss 0.044223\n",
      "Epoch 2\n",
      "\n",
      "Loss 0.040811\n",
      "Loss 0.042088\n",
      "Loss 0.039356\n",
      "Loss 0.041752\n",
      "Loss 0.038526\n",
      "Loss 0.036258\n",
      "Loss 0.034558\n",
      "Loss 0.035449\n",
      "Loss 0.034542\n",
      "Loss 0.034564\n",
      "Epoch 3\n",
      "\n",
      "Loss 0.032737\n",
      "Loss 0.033756\n",
      "Loss 0.032830\n",
      "Loss 0.035932\n",
      "Loss 0.035225\n",
      "Loss 0.032760\n",
      "Loss 0.031617\n",
      "Loss 0.033386\n",
      "Loss 0.032351\n",
      "Loss 0.031994\n",
      "Epoch 4\n",
      "\n",
      "Loss 0.031372\n",
      "Loss 0.031610\n",
      "Loss 0.030776\n",
      "Loss 0.034300\n",
      "Loss 0.033631\n",
      "Loss 0.031517\n",
      "Loss 0.030201\n",
      "Loss 0.032345\n",
      "Loss 0.031918\n",
      "Loss 0.030611\n",
      "Epoch 5\n",
      "\n",
      "Loss 0.029654\n",
      "Loss 0.030340\n",
      "Loss 0.029396\n",
      "Loss 0.033030\n",
      "Loss 0.032457\n",
      "Loss 0.030728\n",
      "Loss 0.028814\n",
      "Loss 0.031569\n",
      "Loss 0.031438\n",
      "Loss 0.029833\n",
      "Epoch 6\n",
      "\n",
      "Loss 0.028528\n",
      "Loss 0.029522\n",
      "Loss 0.028525\n",
      "Loss 0.032260\n",
      "Loss 0.031947\n",
      "Loss 0.030146\n",
      "Loss 0.028207\n",
      "Loss 0.030933\n",
      "Loss 0.030850\n",
      "Loss 0.029240\n",
      "Epoch 7\n",
      "\n",
      "Loss 0.028186\n",
      "Loss 0.028794\n",
      "Loss 0.027447\n",
      "Loss 0.031382\n",
      "Loss 0.031445\n",
      "Loss 0.029310\n",
      "Loss 0.027635\n",
      "Loss 0.030020\n",
      "Loss 0.030281\n",
      "Loss 0.028273\n",
      "Epoch 8\n",
      "\n",
      "Loss 0.027844\n",
      "Loss 0.027658\n",
      "Loss 0.026105\n",
      "Loss 0.029755\n",
      "Loss 0.030098\n",
      "Loss 0.028121\n",
      "Loss 0.026631\n",
      "Loss 0.029007\n",
      "Loss 0.028421\n",
      "Loss 0.027333\n",
      "Epoch 9\n",
      "\n",
      "Loss 0.027428\n",
      "Loss 0.026712\n",
      "Loss 0.025547\n",
      "Loss 0.028827\n",
      "Loss 0.029011\n",
      "Loss 0.027401\n",
      "Loss 0.026082\n",
      "Loss 0.028526\n",
      "Loss 0.027441\n",
      "Loss 0.026709\n",
      "Epoch 10\n",
      "\n",
      "Loss 0.027032\n",
      "Loss 0.026196\n",
      "Loss 0.025164\n",
      "Loss 0.028318\n",
      "Loss 0.028412\n",
      "Loss 0.026959\n",
      "Loss 0.025512\n",
      "Loss 0.028175\n",
      "Loss 0.026809\n",
      "Loss 0.026350\n",
      "Epoch 11\n",
      "\n",
      "Loss 0.026166\n",
      "Loss 0.025877\n",
      "Loss 0.024915\n",
      "Loss 0.027875\n",
      "Loss 0.028070\n",
      "Loss 0.026726\n",
      "Loss 0.025262\n",
      "Loss 0.027979\n",
      "Loss 0.026321\n",
      "Loss 0.026027\n",
      "Epoch 12\n",
      "\n",
      "Loss 0.026003\n",
      "Loss 0.025653\n",
      "Loss 0.024699\n",
      "Loss 0.027610\n",
      "Loss 0.027598\n",
      "Loss 0.026534\n",
      "Loss 0.025055\n",
      "Loss 0.027679\n",
      "Loss 0.025966\n",
      "Loss 0.025804\n",
      "Epoch 13\n",
      "\n",
      "Loss 0.025926\n",
      "Loss 0.025485\n",
      "Loss 0.024508\n",
      "Loss 0.027354\n",
      "Loss 0.027315\n",
      "Loss 0.026482\n",
      "Loss 0.024941\n",
      "Loss 0.027477\n",
      "Loss 0.025571\n",
      "Loss 0.025610\n",
      "Epoch 14\n",
      "\n",
      "Loss 0.025528\n",
      "Loss 0.025290\n",
      "Loss 0.024366\n",
      "Loss 0.027139\n",
      "Loss 0.026970\n",
      "Loss 0.026251\n",
      "Loss 0.024890\n",
      "Loss 0.027239\n",
      "Loss 0.025276\n",
      "Loss 0.025417\n",
      "Epoch 15\n",
      "\n",
      "Loss 0.025424\n",
      "Loss 0.025103\n",
      "Loss 0.024195\n",
      "Loss 0.027021\n",
      "Loss 0.026664\n",
      "Loss 0.026151\n",
      "Loss 0.024624\n",
      "Loss 0.027084\n",
      "Loss 0.024893\n",
      "Loss 0.025298\n",
      "Epoch 16\n",
      "\n",
      "Loss 0.025142\n",
      "Loss 0.024836\n",
      "Loss 0.024054\n",
      "Loss 0.026711\n",
      "Loss 0.026088\n",
      "Loss 0.025925\n",
      "Loss 0.024257\n",
      "Loss 0.026744\n",
      "Loss 0.024434\n",
      "Loss 0.025005\n",
      "Epoch 17\n",
      "\n",
      "Loss 0.024464\n",
      "Loss 0.024111\n",
      "Loss 0.023708\n",
      "Loss 0.026455\n",
      "Loss 0.025586\n",
      "Loss 0.025398\n",
      "Loss 0.023830\n",
      "Loss 0.026330\n",
      "Loss 0.024095\n",
      "Loss 0.024844\n",
      "Epoch 18\n",
      "\n",
      "Loss 0.023943\n",
      "Loss 0.023456\n",
      "Loss 0.023481\n",
      "Loss 0.026157\n",
      "Loss 0.025203\n",
      "Loss 0.025109\n",
      "Loss 0.023597\n",
      "Loss 0.026054\n",
      "Loss 0.024038\n",
      "Loss 0.024745\n",
      "Epoch 19\n",
      "\n",
      "Loss 0.023785\n",
      "Loss 0.023185\n",
      "Loss 0.023222\n",
      "Loss 0.025995\n",
      "Loss 0.024938\n",
      "Loss 0.025033\n",
      "Loss 0.023360\n",
      "Loss 0.025868\n",
      "Loss 0.023827\n",
      "Loss 0.024622\n",
      "Epoch 20\n",
      "\n",
      "Loss 0.023470\n",
      "Loss 0.022995\n",
      "Loss 0.023071\n",
      "Loss 0.025883\n",
      "Loss 0.024714\n",
      "Loss 0.024887\n",
      "Loss 0.023208\n",
      "Loss 0.025824\n",
      "Loss 0.023626\n",
      "Loss 0.024470\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "model = Sharp().to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-8\n",
    ")\n",
    "\n",
    "for i in range(epochs):\n",
    "    losses = []\n",
    "    print(f\"Epoch {i+1}\\n\")\n",
    "\n",
    "    for batch, (image, _) in enumerate(train_dataloader):\n",
    "        flattened = torch.flatten(image, start_dim=1).to(device)\n",
    "        pred = model(image.to(device))\n",
    "\n",
    "        # plt.imshow(np.reshape(pred[0].cpu().detach().numpy(), (28, 28)))\n",
    "        # plt.show()\n",
    "\n",
    "        loss = loss_fn(pred, flattened)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss = loss.item()\n",
    "            # plt.imshow(np.reshape(pred[2].cpu().detach().numpy(), (28, 28)))\n",
    "            # plt.show()\n",
    "            print(f\"Loss {loss:>7f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model_final.pt\")\n",
    "print(\"done\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ac85e60c98da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-1c8305e267c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x128)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD7CAYAAABnoJM0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUbElEQVR4nO3df7DddX3n8efL/GIJ/ggSWTYkBGv8Ea0t9hbpsLulg2CkI3HGqmFGGzu02bHQVet0y9pp7eDatbbq1i1dzdZU6lhQ0am3LS5DI5R2l2BuKoIEkZiqJEWCgKADBgLv/eN8ufeca27uyc2959zk+3zM3Lmfz/fz/Z7zzndy7+t+vr9OqgpJUns9Y9gFSJKGyyCQpJYzCCSp5QwCSWo5g0CSWs4gkKSWMwg0Y0nWJbkrya4klw27HkkzE+8j0EwkWQB8AzgP2ANsBy6qqp1DLUzSYVs47AJ01DoT2FVVuwGSXA2sBw4aBIuzpI5j6QDL01R+wEPfq6rlw65D84dBoJlaAdzT1d8DvHKqlY9jKa/MuXNelKb393XNt4ddg+YXg0BzJskmYBPAcRw/5GokTcWTxZqpvcDKrv6pzbJxVbW5qkaqamQRSwZanKT+GQSaqe3AmiSnJ1kMbABGh1yTpBnw0JBmpKoOJLkUuA5YAGypqjuGXJakGTAINGNVdS1w7bDrkHRkPDQkSS1nEEhSyxkEktRyBoEktZxBIEktZxBIUssZBJLUcgaBJLWcQSBJLWcQSFLLGQSS1HIGgSS1nEEgSS1nEEhSyxkEktRyBoEktZxBIEktZxBIUsv5UZU6aj3waz/X01/1ll3j7a/vO7ln7PH9i8bbK65a1DN2/J4fjrefunXnbJYoHRWcEUhSyxkEktRyHhrSUeu//NZf9fRfv/Shic5PHGLDc3q73zrw6Hj7T+7/hSMv7DB9ed9p4+2lH3x2z9jCrTsGXY5ayBmBJLWcQSBJLWcQSFLLeY5AR62PvHtDT//3Xj7xd82yO6tn7KGXZLy9+OXf7xn7wMs+P97+8Cm39Iz93aMnjLd/8fgf0q/H6vGe/i37l463zznuid6Vu97zBW/6Tz1DL9za91tKM+aMQIeUZEuSfUm+1rXsxCTXJ7m7+b5smDVKOjIGgabzCWDdpGWXAVurag2wtelLOkp5aEiHVFU3JVk9afF6Ji7CvBK4EfjtwVXVsfSaWyb1p173WYd4nf/5b88Zb/+3s1f3bvcPE3crf+CcF/Rd28LHnuqt7bZ7x9vPvelzPWM/uXjiTufjv9V717M0CM4INBMnV9XTv9m+C5x8qJUlzW8GgY5IVRVQBxtLsinJWJKxJ9g/4Mok9csg0Ezcl+QUgOb7voOtVFWbq2qkqkYWsWSgBUrqn+cINBOjwEbg/c33Lwy3nCNz4Lv3jbeXfu6+nrEnu9pLr3lgxu9x369OPCn1pYt7f+z++MEXjbdX/8Xu3tpm/I5S/5wR6JCSXAXcDLwoyZ4kF9MJgPOS3A28qulLOko5I9AhVdVFUwydO9BCJM0Zg0CaAwtPW9nT/9N3/+l4e1EW9Ix99k9eNd5+7r03z21h0kF4aEiSWs4gkKSWMwgkqeU8RyDNga+/c0VP/2eXTDz99I7HH+sZO3Hno0jD5IxAklrOIJCklvPQkDRL9v/iz463//mXPjxpdOIRG297+9t7Rv7N//vyXJYlTcsZgSS1nEEgSS1nEEhSy3mOQJol33nNxN9VJ6T3sdsX/ct54+3j/89Xe8YO+mEO0gA5I5CkljMIJKnlDAJJajnPEUgz9IxnPrOn/5b/8E/j7Uee+lHP2L4/eP54e8n+7XNbmHSYnBFIUssZBJLUch4akmbo7t9/aU//b0/6s/H2+rtf3zO25FoPB2n+ckYgSS1nEEhSyxkEktRyniOQ+vTwm8/q6d/2po/09L954Inx9g//8NSesSXcO3eFSUfIGYEktZxBIEkt56Eh6RAWrvh34+13/O6ne8aWpPfHZ8NX3zLeXv5FLxfV0cMZgSS1nEGgQ0qyMskNSXYmuSPJ25vlJya5Psndzfdlw65V0swYBJrOAeBdVbUWOAu4JMla4DJga1WtAbY2fUlHIc8R6JCq6l7oXPtYVT9IciewAlgPnNOsdiVwI/DbQyhxVmVh74/ET/3tnvH2G054oGfsUz94Xk//5N+d+LvqqTmoTZorzgjUtySrgTOAW4CTm5AA+C5w8rDqknRkDAL1JckJwOeAd1TVI91jVVUc5KN3k2xKMpZk7An2D6hSSYfLQ0OaVpJFdELgU1X1+WbxfUlOqap7k5wC7Ju8XVVtBjYDPCsnHh2f0f5TL+rpvvd5n5xy1Sv+4A09/ed89eY5KUmaa84IdEhJAnwcuLOqPtQ1NApsbNobgS8MujZJs8MZgaZzNvAW4PYktzbL3g28H/hMkouBbwNvHE55ko6UQaBDqqp/AjLF8LmDrEXS3DAI1HoL1r5wvL3p6qmPcK3dcklPf/Unt81ZTdIgeY5AklrOIJCklvPQkFrv678+8Zik1x7/yJTrnXrj470L6ui4IlaajjMCSWo5g0CSWs4gkKSW8xyBWudHrz2zp7/1tR/s6h0/2GKkecAZgSS1nEEgSS3noSG1zr+evaCnv2rh1IeDuj98ZtEjvZePevGojhXOCCSp5QwCSWo5g0CSWs5zBFKX//7A2p7+za9ePd6ue28fcDXSYDgjkKSWMwgkqeVSPkFRA/CsnFivjB9oNh/8fV2zo6pGhl2H5g9nBJLUcgaBJLWcQSBJLec5Ag1EkvuBbwMnAd8bcjlPm0+1wODqOa2qlg/gfXSUMAg0UEnG5suJyvlUC8y/etQeHhqSpJYzCCSp5QwCDdrmYRfQZT7VAvOvHrWE5wgkqeWcEUhSyxkEGpgk65LclWRXkssG/N5bkuxL8rWuZScmuT7J3c33ZQOqZWWSG5LsTHJHkrcPsx7JINBAJFkAXAG8BlgLXJRk7aG3mlWfANZNWnYZsLWq1gBbm/4gHADeVVVrgbOAS5p9Max61HIGgQblTGBXVe2uqseBq4H1g3rzqroJeHDS4vXAlU37SuB1A6rl3qr656b9A+BOYMWw6pEMAg3KCuCerv6eZtkwnVxV9zbt7wInD7qAJKuBM4Bb5kM9aieDQAKqc/ncQC+hS3IC8DngHVX1yLDrUXsZBBqUvcDKrv6pzbJhui/JKQDN932DeuMki+iEwKeq6vPDrkftZhBoULYDa5KcnmQxsAEYHXJNo8DGpr0R+MIg3jRJgI8Dd1bVh4Zdj+QNZRqYJBcA/wNYAGypqvcN8L2vAs6h84TP+4D3AH8NfAZYRefJqG+sqsknlOeiln8P/CNwO/BUs/jddM4TDLweySCQpJab9tDQwW7EmTSeJB9pbhK6LckrusY2NjfH3J1k48G2lyQNVz/nCD7Bj9+I0+01wJrmaxPwv6BzlySd6fcr6VxD/h7vlJSk+WfaIJjiRpxu64G/rI5twHOaKx5eDVxfVQ9W1UPA9Rw6UCRJQzAbVw1NdaPQfLyBSJI0ycJhFwCQZBOdw0osXbr0Z1784hcPuSJJOrrs2LHjezP9LOrZCIKpbhTaS+dyve7lNx7sBapqM82HcoyMjNTY2NgslCVJ7ZHk2zPddjYODY0Cv9xcPXQW8HDzvJTrgPOTLGtOEp/fLJMkzSPTzgi6b8RJsofOlUCLAKrqo8C1wAXALuBR4FeasQeTvJfOHaUAl3tzjCTNP9MGQVVdNM14AZdMMbYF2DKz0iRJg+CzhiSp5QwCSWo5g0CSWs4gkKSWMwgkqeUMAklqOYNAklrOIJCkljMIJKnlDAJJajmDQJJaziCQpJYzCCSp5QwCSWo5g0CSWs4gkKSW6ysIkqxLcleSXUkuO8j4h5Pc2nx9I8n3u8ae7BobncXaJUmzoJ+PqlwAXAGcB+wBticZraqdT69TVe/sWv83gDO6XuKxqvrpWatYkjSr+pkRnAnsqqrdVfU4cDWw/hDrXwRcNRvFSZLmXj9BsAK4p6u/p1n2Y5KcBpwOfKlr8XFJxpJsS/K6mRYqSZob0x4aOkwbgGuq6smuZadV1d4kzwe+lOT2qvpm90ZJNgGbAFatWjXLJUmSDqWfGcFeYGVX/9Rm2cFsYNJhoara23zfDdxI7/mDp9fZXFUjVTWyfPnyPkqSJM2WfoJgO7AmyelJFtP5Zf9jV/8keTGwDLi5a9myJEua9knA2cDOydtKkoZn2kNDVXUgyaXAdcACYEtV3ZHkcmCsqp4OhQ3A1VVVXZu/BPhYkqfohM77u682kiQNX3p/bw/fyMhIjY2NDbsMSTqqJNlRVSMz2dY7iyWp5QwCSWo5g0CSWs4gkKSWMwgkqeUMAklqOYNAklrOIJCkljMIJKnlDAJJajmDQJJaziCQpJYzCCSp5QwCSWo5g0CSWq6vIEiyLsldSXYluewg429Ncn+SW5uvX+0a25jk7uZr42wWL0k6ctN+QlmSBcAVwHnAHmB7ktGDfNLYp6vq0knbngi8BxgBCtjRbPvQrFQvSTpi/cwIzgR2VdXuqnocuBpY3+frvxq4vqoebH75Xw+sm1mpkqS50E8QrADu6ervaZZN9voktyW5JsnKw9xWkjQks3Wy+G+A1VX1cjp/9V95OBsn2ZRkLMnY/fffP0slSZL60U8Q7AVWdvVPbZaNq6oHqmp/0/1z4Gf63bbZfnNVjVTVyPLly/utXZI0C/oJgu3AmiSnJ1kMbABGu1dIckpX90LgzqZ9HXB+kmVJlgHnN8skSfPEtFcNVdWBJJfS+QW+ANhSVXckuRwYq6pR4D8nuRA4ADwIvLXZ9sEk76UTJgCXV9WDc/DvkCTNUKpq2DX0GBkZqbGxsWGXIUlHlSQ7qmpkJtt6Z7EktZxBIEktZxBIUssZBJLUcgaBJLWcQSBJLWcQSFLLGQSS1HIGgSS1nEEgSS1nEEhSyxkEktRyBoEktZxBIEktZxBIUssZBJLUcn0FQZJ1Se5KsivJZQcZ/80kO5PclmRrktO6xp5McmvzNTp5W0nScE37UZVJFgBXAOcBe4DtSUaramfXal8BRqrq0SRvAz4AvKkZe6yqfnp2y5YkzZZ+ZgRnAruqandVPQ5cDazvXqGqbqiqR5vuNuDU2S1TkjRX+gmCFcA9Xf09zbKpXAx8sat/XJKxJNuSvO7wS5QkzaVpDw0djiRvBkaAn+9afFpV7U3yfOBLSW6vqm9O2m4TsAlg1apVs1mSJGka/cwI9gIru/qnNst6JHkV8DvAhVW1/+nlVbW3+b4buBE4Y/K2VbW5qkaqamT58uWH9Q+QJB2ZfoJgO7AmyelJFgMbgJ6rf5KcAXyMTgjs61q+LMmSpn0ScDbQfZJZkjRk0x4aqqoDSS4FrgMWAFuq6o4klwNjVTUK/BFwAvDZJADfqaoLgZcAH0vyFJ3Qef+kq40kSUOWqhp2DT1GRkZqbGxs2GVI0lElyY6qGpnJtt5ZLEktZxBIUssZBJLUcgaBJLWcQSBJLWcQSFLLGQSS1HIGgSS1nEEgSS1nEEhSyxkEktRyBoEktZxBIEktZxBIUssZBJLUcgaBJLVcX0GQZF2Su5LsSnLZQcaXJPl0M35LktVdY/+1WX5XklfPYu2SpFkwbRAkWQBcAbwGWAtclGTtpNUuBh6qqhcAHwb+sNl2LZ3POH4psA74s+b1JEnzRD8zgjOBXVW1u6oeB64G1k9aZz1wZdO+Bjg3nQ8vXg9cXVX7q+pfgF3N60mS5ol+gmAFcE9Xf0+z7KDrVNUB4GHguX1uK0kaooXDLgAgySZgU9Pdn+Rrw6xnHjkJ+N6wi5gn3BcT3BcT3BcTXjTTDfsJgr3Ayq7+qc2yg62zJ8lC4NnAA31uS1VtBjYDJBmrqpF+/wHHMvfFBPfFBPfFBPfFhCRjM922n0ND24E1SU5PspjOyd/RSeuMAhub9i8BX6qqapZvaK4qOh1YA3x5psVKkmbftDOCqjqQ5FLgOmABsKWq7khyOTBWVaPAx4FPJtkFPEgnLGjW+wywEzgAXFJVT87Rv0WSNAN9nSOoqmuBayct+72u9o+AN0yx7fuA9x1GTZsPY91jnftigvtigvtigvtiwoz3RTpHcCRJbeUjJiSp5YYWBEfy2IpjTR/74jeT7ExyW5KtSU4bRp2DMN2+6Frv9UkqyTF7xUg/+yLJG5v/G3ck+atB1zgoffyMrEpyQ5KvND8nFwyjzrmWZEuSfVNdYp+OjzT76bYkr+jrhatq4F90Tjp/E3g+sBj4KrB20jq/Dny0aW8APj2MWufJvvgF4Pim/bY274tmvWcCNwHbgJFh1z3E/xdrgK8Ay5r+84Zd9xD3xWbgbU17LfCtYdc9R/viPwKvAL42xfgFwBeBAGcBt/TzusOaERzJYyuONdPui6q6oaoebbrb6NyPcSzq5/8FwHvpPM/qR4MsbsD62Re/BlxRVQ8BVNW+Adc4KP3siwKe1bSfDfzrAOsbmKq6ic6VmVNZD/xldWwDnpPklOled1hBcCSPrTjWHO5jOC6mk/jHomn3RTPVXVlVfzfIwoagn/8XLwRemOT/JtmWZN3AqhusfvbF7wNvTrKHzhWOvzGY0uadGT3WZ148YkL9SfJmYAT4+WHXMgxJngF8CHjrkEuZLxbSOTx0Dp1Z4k1JfrKqvj/MoobkIuATVfXBJD9H576ml1XVU8Mu7GgwrBnB4Ty2gkmPrTjW9PUYjiSvAn4HuLCq9g+otkGbbl88E3gZcGOSb9E5Bjp6jJ4w7uf/xR5gtKqeqM7Tfb9BJxiONf3si4uBzwBU1c3AcXSeQ9Q2ff0+mWxYQXAkj6041ky7L5KcAXyMTggcq8eBYZp9UVUPV9VJVbW6qlbTOV9yYVXN+Bkr81g/PyN/TWc2QJKT6Bwq2j3AGgeln33xHeBcgCQvoRME9w+0yvlhFPjl5uqhs4CHq+re6TYayqGhOoLHVhxr+twXfwScAHy2OV/+naq6cGhFz5E+90Ur9LkvrgPOT7ITeBL4rao65mbNfe6LdwH/O8k76Zw4fuux+IdjkqvohP9JzfmQ9wCLAKrqo3TOj1xA57NfHgV+pa/XPQb3lSTpMHhnsSS1nEEgSS1nEEhSyxkEktRyBoEktZxBIEktZxBIUssZBJLUcv8fMG3mktWqGjIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for batch, (image, _) in enumerate(test_dataloader):\n",
    "    f, ax = plt.subplots(2, 1)\n",
    "    ax[0].imshow(image[0][0])\n",
    "    pred = model(image[0][0].to(device))\n",
    "    pred = np.reshape(pred, (28, 28))\n",
    "    ax[1].imshow()\n",
    "    plt.show()\n",
    "    # flattened = torch.flatten(image[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
